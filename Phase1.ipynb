{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b06eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import Accuracy, F1Score, Precision, Recall\n",
    "from keras.optimizers import SGD, Adam, Adagrad, RMSprop, Nadam, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory_path = r'/content/drive/MyDrive/31 dekabr/Deep Learning'\n",
    "if os.path.isdir(directory_path):\n",
    "    folder_contents = os.listdir(directory_path)\n",
    "    print(f\"Contents of '{directory_path}':\")\n",
    "    for item in folder_contents:\n",
    "        print(item)\n",
    "else:\n",
    "    print(f\"Error: '{directory_path}' is not a valid directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "DATA_DIR = r\"/content/drive/MyDrive/31 dekabr/Deep Learning\"\n",
    "IMG_SIZE = (224, 168)\n",
    "BATCH = 32\n",
    "SEED = 42\n",
    "\n",
    "# 1) Train split\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,# 80%% train\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    color_mode=\"rgb\"# ensures (H,W,3)\n",
    ")\n",
    "\n",
    "#2) Validation split\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,# same split rule\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    color_mode=\"rgb\"\n",
    ")\n",
    "\n",
    "print(\"Classes:\", train_ds.class_names)\n",
    "num_classes = len(train_ds.class_names)\n",
    "\n",
    "# 3) Normalize\n",
    "train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "val_ds   = val_ds.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "\n",
    "#4) Speed, this is not thatimportant but recommended\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.losses import loss\n",
    "from keras.src.optimizers import optimizer\n",
    "model= keras.Sequential([layers.Input(shape=(224,168,3)),#224,168,3\n",
    "                  layers.Conv2D(filters=32,kernel_size=(3,3), padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\",strides=(1,1)),#parameter size is 3x3x3x32 +32bias=896 parameters in 1st layer\n",
    "                  layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),#112x84x32 <- this is the output size,\n",
    "                  layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\"),#3x3x32x64+64=18496\n",
    "                  layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),#56x42x64 <-output,\n",
    "                  layers.Conv2D(filters=128,kernel_size=(3,3), strides=(1,1),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\"),#3x3x64x128+128=73856\n",
    "                  layers.MaxPool2D(pool_size=(2,2),strides=(2,2)), #28x21x128<-outpute\n",
    "                  # layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\", strides=(1,1)),\n",
    "                  # layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "                  # layers.Conv2D(filters=512,kernel_size=(3,3), padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\",strides=(1,1)),\n",
    "                  # layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "                  # layers.Conv2D(filters=1024,kernel_size=(3,3), padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\",strides=(1,1)),\n",
    "                  # layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "                  # layers.Conv2D(filters=2048,kernel_size=(3,3), padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\",strides=(1,1)),\n",
    "                  # layers.MaxPool2D(pool_size=(2,2),strides=(2,3)),\n",
    "                  # layers.Conv2D(filters=4096,kernel_size=(3,3), padding=\"same\",activation=\"relu\",kernel_initializer=\"he_normal\",strides=(1,1)),\n",
    "                  # layers.MaxPool2D(pool_size=(2,2),strides=(2,1)),#19x19 after this maxpool\n",
    "                  layers.Flatten(),#75264 input dimensions\n",
    "                  # layers.Dense(512,activation=\"relu\",kernel_initializer=\"he_normal\"),#512x361+512bias\n",
    "                  # layers.Dense(1024,activation=\"relu\",kernel_initializer=\"he_normal\"),#1024x512+1024\n",
    "                  layers.Dense(64,activation=\"relu\",kernel_initializer=\"he_normal\"),#64x75264 + 64 bias=4816960 parameters in the 1st layer of te MLP\n",
    "                  layers.Dense(21,activation=\"softmax\")])#21x64 +21 bias = 1365 PARAMETERS IN THE LAST LAYER\n",
    "model.compile(optimizer=SGD(learning_rate=0.0001,momentum=0.9, nesterov=True),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early=EarlyStopping(monitor=\"val_loss\",\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=30,\n",
    "          callbacks=early)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e074e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(val_ds)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
